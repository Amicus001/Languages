{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "\n",
    "data1 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name='BJY', header= 1)\n",
    "data2 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name='LHG_Neapolitan', header= 1)\n",
    "data3 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name= 'LHG_Horror', header= 1)\n",
    "data4 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name= 'SY', header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>FROM</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>\"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CLASS TYPE FROM                                               TEXT\n",
       "0     H    E    K  \"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...\n",
       "1     H    E    K  또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...\n",
       "2     H    E    K  그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...\n",
       "3     H    E    K  여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...\n",
       "4     H    E    K  온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 괴담 데이터 하나의 데이터프레임으로 합치기\n",
    "\n",
    "horror = pd.concat([data1, data2, data3, data4], axis = 0)\n",
    "horror.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TYPE</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>\"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TYPE                                               TEXT\n",
       "0    E  \"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...\n",
       "1    E  또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...\n",
       "2    E  그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...\n",
       "3    E  여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...\n",
       "4    E  온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#나는 type만 사용할 거니까 나머지는 제거\n",
    "horror = horror.drop(['CLASS', 'FROM'], axis = 1)\n",
    "\n",
    "horror.head() #없어져땅. 굿~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punc = string.punctuation\n",
    "\n",
    "# 정규표현식 패턴만들기\n",
    "hangle_pattern = \"[^ㄱ-ㅎㅏ-ㅣ가-힣]\"\n",
    "num_pattern = \"[0-9]\"\n",
    "eng_pattern = \"[a-zA-Z]\"\n",
    "\n",
    "# 한글, 숫자, 영어만 남기기\n",
    "horror['TEXT'] = horror['TEXT'].str.replace(hangle_pattern, '', regex=True)\n",
    "horror['TEXT'] = horror['TEXT'].str.replace(num_pattern, '', regex=True)\n",
    "horror['TEXT'] = horror['TEXT'].str.replace(eng_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 문장 있으면 제거\n",
    "import numpy as np\n",
    "horror['TEXT'].replace('', np.nan, inplace=True)\n",
    "horror = horror.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 사전 불러오기\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "filename = '../datas/kr_stopwords.txt'\n",
    "url = \"https://gist.githubusercontent.com/chulgil/d10b18575a73778da4bc83853385465c/raw/a1a451421097fa9a93179cb1f1f0dc392f1f9da9/stopwords.txt\"\n",
    "ret = urlretrieve(url, filename)\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할하기(train, test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(horror, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TYPE    0\n",
       "TEXT    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isin(stopwords).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2560, 2), (640, 2))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "['관리', '실', '에서', '여성', '직원', '이', '당신', '의', '휴대폰', '으로', '어디', '로', '와', '달라', '는', '등', '의', '요청', '이', '왔다면', '바로', '끊으십시오', '관리', '실', '에선', '어떤', '경우', '에서도', '당신', '의', '휴대폰', '에', '직접', '전화', '를', '하지', '않을것이며', '관리', '실', '에서는', '여성', '직원', '을', '고용', '하', '지', '않습니다']\n"
     ]
    }
   ],
   "source": [
    "for sentence in train['TEXT']:\n",
    "    print(type(sentence))\n",
    "    print(okt.morphs(sentence))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이즈 이후 불용어 제거하기\n",
    "train_tokens = [okt.morphs(sentence)for sentence in train['TEXT']] # tokenize\n",
    "train_tokens = [[token for token in sent if token not in stopwords]for sent in train_tokens] # extract stopwords\n",
    "test_tokens = [okt.morphs(sentence)for sentence in test['TEXT']] #tokenize\n",
    "test_tokens = [[token for token in sent if token not in stopwords]for sent in test_tokens] #extract stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train = train_tokens\n",
    "backup_test = test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['할아버지',\n",
       " '돌아가신',\n",
       " '후난',\n",
       " '추억',\n",
       " '떠올리며',\n",
       " '유품',\n",
       " '정리',\n",
       " '하고',\n",
       " '있었',\n",
       " '다그',\n",
       " '오래된',\n",
       " '포르노',\n",
       " '컬렉션',\n",
       " '살펴보던난',\n",
       " '내게',\n",
       " '여동생',\n",
       " '있었다는',\n",
       " '사실',\n",
       " '깨닫게',\n",
       " '되었다']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokens = backup_train\n",
    "# test_tokens = backup_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성하기\n",
    "vocab = {'<PAD>': 8000, '<UNK>':7999}\n",
    "for sen in train_tokens:\n",
    "    for word in sen:\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "        \n",
    "# 집합으로 변환하여 패딩 값 추가 \n",
    "\n",
    "# vocab2 = {}\n",
    "# for sen in test_tokens:\n",
    "#     for word in sen:\n",
    "#         if word is not None:  # None이 아닌 경우에만 처리\n",
    "#             if word in vocab:\n",
    "#                 vocab2[word] += 1\n",
    "#             else:\n",
    "#                 vocab2[word] = 1\n",
    "\n",
    "# 결과 확인\n",
    "# 패드가 없어서 계속 none 값이 섞이길래 강제로 추가했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD>', 8000),\n",
       " ('<UNK>', 7999),\n",
       " ('는', 1919),\n",
       " ('은', 1629),\n",
       " ('한', 1121),\n",
       " ('도', 714),\n",
       " ('내', 541),\n",
       " ('다', 513),\n",
       " ('말', 490),\n",
       " ('하고', 456)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD>', 8000),\n",
       " ('<UNK>', 7999),\n",
       " ('는', 1919),\n",
       " ('은', 1629),\n",
       " ('한', 1121),\n",
       " ('도', 714),\n",
       " ('내', 541),\n",
       " ('다', 513),\n",
       " ('말', 490),\n",
       " ('하고', 456)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 처리 한번 더,,,,,,,,,,\n",
    "clean = {token: value for token,value in vocab.items() if token not in stopwords}\n",
    "\n",
    "sorted(clean.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 / 디코딩 딕셔너리 생성하기\n",
    "\n",
    "encode = {token: idx for idx, token in enumerate(vocab)}\n",
    "decode = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "UNK = encode.get('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 인코딩하기\n",
    "\n",
    "train_id = [[encode.get(token, UNK) for token in sen]for sen in train_tokens]\n",
    "test_id = [[encode.get(token,UNK) for token in sen] for sen in test_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560, 640)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_id), len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[720,\n",
       " 722,\n",
       " 1,\n",
       " 9105,\n",
       " 1,\n",
       " 2132,\n",
       " 1840,\n",
       " 435,\n",
       " 3851,\n",
       " 283,\n",
       " 6905,\n",
       " 1,\n",
       " 4173,\n",
       " 1,\n",
       " 858,\n",
       " 2057,\n",
       " 5503,\n",
       " 941,\n",
       " 3824,\n",
       " 244]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAD\n",
    "\n",
    "def pad_sequences(sequences, max_len, pad_token):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if seq is None:  # 시퀀스가 None일 경우 패딩하지 않고 건너뜁니다.\n",
    "            continue\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [pad_token] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded.append(seq)\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_ID: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 2, 3, 12, 13, 14, 6, 15, 16, 17, 18, 2, 3, 19, 4, 5, 20, 21, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "\n",
      " TEST_ID: [720, 722, 1, 9105, 1, 2132, 1840, 435, 3851, 283, 6905, 1, 4173, 1, 858, 2057, 5503, 941, 3824, 244, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = max([len(sen) for sen in train_id + test_id])\n",
    "PAD_ID = encode.get('<PAD>')\n",
    "\n",
    "train_id = pad_sequences(train_id, MAX_LEN, PAD_ID)\n",
    "test_id = pad_sequences(test_id, MAX_LEN, PAD_ID)\n",
    "\n",
    "print(f'TRAIN_ID: {train_id[0]},\\n\\n TEST_ID: {test_id[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 모델 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 train, test의 label과 text 나누기\n",
    "xtrain = np.array(train_id)\n",
    "ytrain = np.array(train['TYPE'])\n",
    "xtest = np.array(test_id)\n",
    "ytest = np.array(test['TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  720,   722,     1, ...,     0,     0,     0],\n",
       "       [  867, 20207,   344, ...,     0,     0,     0],\n",
       "       [  510,  7710,  8296, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    1,  1248,  1818, ...,     0,     0,     0],\n",
       "       [ 5707,     1, 15602, ...,     0,     0,     0],\n",
       "       [ 6533,  1513,     9, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['G', 'G', 'E', ..., 'G', 'G', 'E'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨이 정수가 아니라서 계속 오류가 생긴다. 제기랄.\n",
    "# 라벨인코더도 안 먹어서 내가 그냥 바꿔주도록 하자.\n",
    "label_to_index = {'G': 0, 'E': 1}\n",
    "\n",
    "# ytrain 인코딩\n",
    "y_train = [label_to_index[label] for label in train['TYPE']]\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "# ytest 인코딩\n",
    "y_test = [label_to_index[label] for label in test['TYPE']]\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "xtrain = torch.LongTensor(xtrain)\n",
    "xtest = torch.LongTensor(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23943"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어사전 크기 계산하기\n",
    "vocab_num = len(encode) + 1 #패딩 크기(1) 계산하기\n",
    "\n",
    "vocab_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN\n",
    "\n",
    "# class RNNClassifier(nn.Module):\n",
    "#     def __init__(self, n_vocab, output_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(n_vocab, embedding_dim)\n",
    "#         self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         embedded = self.embedding(x)\n",
    "#         output, hidden = self.rnn(embedded)\n",
    "#         output = self.fc(output[:,-1,:])\n",
    "#         return output\n",
    "        \n",
    "# model = RNNClassifier(vocab_num, output_size=2, embedding_dim=64, hidden_dim=128, n_layers=2, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 출력 크기를 1로 변경 (이진 분류 문제)\n",
    "        self.sigmoid = nn.Sigmoid()  # 시그모이드 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = self.sigmoid(output)  # 시그모이드 함수 적용\n",
    "        return output\n",
    "\n",
    "# 모델 생성\n",
    "model = RNNClassifier(vocab_num, embedding_dim=64, hidden_dim=128, n_layers=2, dropout=0.5)\n",
    "\n",
    "# 손실함수 및 옵티마이저 정의\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수, 옵티마이저 정의하기\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.,  ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.dtype\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "train_dataset = TensorDataset(xtrain, y_train)\n",
    "# ytest와 동일한 개수의 샘플을 xtest에서 선택하여 새로운 xtest_subset을 만듭니다.\n",
    "xtest_subset = xtest[:len(y_test)]\n",
    "test_dataset = TensorDataset(xtest_subset, y_test)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2560, 288]), (2560,), torch.Size([640, 288]), (640,))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape, xtest_subset.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 720,  722,    1, 9105,    1, 2132, 1840,  435, 3851,  283, 6905,    1,\n",
       "         4173,    1,  858, 2057, 5503,  941, 3824,  244,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'G')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest[0], ytest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for f, t in train_dataset:\n",
    "    print(f.shape, t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.7006478309631348, Accuracy: 53.125%\n",
      "Epoch: 2, Loss: 3.1193807125091553, Accuracy: 53.125%\n",
      "Epoch: 3, Loss: 22.29237937927246, Accuracy: 53.125%\n",
      "Epoch: 4, Loss: 27.685970306396484, Accuracy: 53.125%\n",
      "Epoch: 5, Loss: 32.16034698486328, Accuracy: 53.125%\n",
      "Epoch: 6, Loss: 37.013893127441406, Accuracy: 53.125%\n",
      "Epoch: 7, Loss: 40.64053726196289, Accuracy: 53.125%\n",
      "Epoch: 8, Loss: 41.362422943115234, Accuracy: 53.125%\n",
      "Epoch: 9, Loss: 42.21215057373047, Accuracy: 53.125%\n",
      "Epoch: 10, Loss: 42.31328201293945, Accuracy: 53.125%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(xtrain)\n",
    "    outputs = outputs.squeeze()\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 평가\n",
    "    outputs = model(xtest)\n",
    "    \n",
    "    # y_test를 one-hot 벡터로 변환\n",
    "    # 모델 출력과 정수로 된 레이블 비교\n",
    "    predicted_labels = outputs.argmax(dim=1)\n",
    "    accuracy = (predicted_labels == y_test).float().mean()\n",
    "\n",
    "    \n",
    "    # 모델 출력과 one-hot 레이블 비교\n",
    "    #accuracy = (outputs.argmax(dim=1) == y_one_hot.argmax(dim=1)).float().mean()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}, Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = ['이 수칙서에 5번 문항은 존재하지 않습니다. 5번 문항이 존재하는 수칙서를 배급받았을 시 즉시 찢어버리고, 물가에는 3일간 가지 마세요.']\n",
    "new_token = [okt.morphs(sentence) for sentence in new_data]\n",
    "new_ids = [[encode.get(token, UNK) for token in sen]for sen in new_token]\n",
    "new_ids = pad_sequences(new_ids, MAX_LEN, PAD_ID)\n",
    "new_ids = torch.LongTensor(new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(new_ids)\n",
    "    _, predicted_labels = torch.max(outputs, dim=1)\n",
    "    torch.save(model.state_dict(), f'../savepoint/model_scarystory.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력된 문장 : 이 수칙서에 5번 문항은 존재하지 않습니다. 5번 문항이 존재하는 수칙서를 배급받았을 시 즉시 찢어버리고, 물가에는 3일간 가지 마세요.\n",
      "추리형 괴담입니다.\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(predicted_labels):\n",
    "    print(f'입력된 문장 : {new_data[i]}')\n",
    "    if pred== 0:\n",
    "        print(\"추리형 괴담입니다.\")\n",
    "    else:\n",
    "        print(\"경험담형 괴담입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
