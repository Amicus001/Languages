{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ Soynlp ] 학습 기반 토크나이저 <hr>\n",
    "\n",
    "- 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저\n",
    "- 비지도 학습으로 단어 토큰화 -> 데이터에 자주 등장하는 단어들을 단어로 분석\n",
    "- 내부적으로 단어 점수 표로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '이다', '.']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다']\n"
     ]
    }
   ],
   "source": [
    "# 기존 형태소 분석기: 신조어나 형태소 분석기에 등록되지 않은 단어를 제대로 구분하지 못함\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "print(tokenizer.morphs(phrase='에이비식스 이대휘 1월 최애돌 기부 요정입니다'))\n",
    "\n",
    "#형태소 분석시 매개변수 stem = True 설정. \n",
    "print(tokenizer.morphs(phrase='에이비식스 이대휘 1월 최애돌 기부 요정입니다.', stem=True)) # 어근 찾아 원형 돌려주는 친구\n",
    "print(tokenizer.morphs(phrase='에이비식스 이대휘 1월 최애돌 기부 요정입니다', norm=True))# 정규화시켜줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ Sonlpy ] 사용 : 말뭉치(Corpus) 데이터셋으로 학습 진행 후 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../datas/text_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp import DoublespaceLineCorpus #하나로 통합된 문서 데이터 분리하기\n",
    "from soynlp.word import WordExtractor   # 단어 추출기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 문서 : 30091개\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 문서 분리하기\n",
    "corpus = DoublespaceLineCorpus(corpus_fname=filename)\n",
    "print(f'훈련 데이터 문서 : {len(corpus)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.029 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "#soynlpy 학습 진행하기\n",
    "word_extractor = WordExtractor()\n",
    "\n",
    "# 학습 진행하며 단어별 점수\n",
    "word_extractor.train(sents=corpus)\n",
    "\n",
    "# 단어별 점수표 추출하기\n",
    "word_scores = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] = 필\n",
      "[1] = 룽\n",
      "[2] = 게\n",
      "[3] = 딜\n",
      "[4] = 잔\n",
      "[5] = 새\n",
      "[6] = 첸\n",
      "[7] = 컥\n",
      "[8] = 궂\n",
      "[9] = 텅\n",
      "[10] = 뵙\n",
      "[11] = 너\n",
      "[12] = 줴\n",
      "[13] = 띠\n",
      "[14] = ㄱ\n",
      "[15] = 덥\n",
      "[16] = 팍\n",
      "[17] = 노\n",
      "[18] = 줍\n",
      "[19] = 획\n",
      "[20] = 앵\n",
      "[21] = 얇\n",
      "[22] = 뮐\n",
      "[23] = 텃\n",
      "[24] = 인\n",
      "[25] = 칵\n",
      "[26] = 횃\n",
      "[27] = 순\n",
      "[28] = 진\n",
      "[29] = 갉\n",
      "[30] = 땀\n"
     ]
    }
   ],
   "source": [
    "# 단어별 점수표 확인하기\n",
    "for idx, key in enumerate(iterable= word_scores.keys()):\n",
    "    print(f'[{idx}] = {key}')\n",
    "    if idx == 30:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "응집 확률: 내부 문자열(Substring)이 얼마나 자주 응집하여 나타나는지를 판단하는 척도\n",
    "- 원리 : 문자열을 문자 단위로 분리, 왼쪽부터 순서대로 문자를 추가\n",
    "        각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산 / 누적곱한 값\n",
    "- 값이 높을수록 전체 말뭉치(corpus)에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06393648140409527"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores['바다'].cohesion_forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05007307656055328"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores['바다가'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11518621707955429"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores['바다에'].cohesion_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L Tokenizer <hr>\n",
    "- 띄어쓰기 단위로 나눈 어절 토큰 : L토큰, R 토큰 (공원에 = 공원 + 에, 공부하러 = 공부 + 하러)\n",
    "- 분리 기준 : 점수가 가장 높은 L토큰을 찾아내는 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "l_tokenizer.tokenize('국제사회와 우리의 노력들로 범죄를 척결하자', flatten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 점수 토크나이저\n",
    "- 띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저\n",
    "- 띄어쓰기가 되어있지 않은 문장을 넣어 점수를 통해 토큰화된 결과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize('국제사회와우리의노력들로범죄를척결하자')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOyNlp를 이용한 반복되는 문자 정제하기\n",
    "- ㅠㅠ, ㅋㅋ, ㅎㅎ등의 이모티콘의 경우 불필요하게 연속되는 경우 많음\n",
    "- ㅋㅋ, ㅋㅋㅋ, ㅋ, ㅋㅋㅋㅋ 등의 경우를 모두 서로 다른 단어로 처리하는 것은 불필요\n",
    "--> 반복되는 문자는 하나로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "아ㅋ 이영화존잼쓰ㅜ\n",
      "아ㅋㅋ 이영화존잼쓰ㅜㅜ\n",
      "아ㅋㅋ 이영화존잼쓰ㅜㅜ\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋ 이영화존잼쓰ㅜㅜㅜㅜ', num_repeats=1))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ 이영화존잼쓰ㅜㅜㅜㅜㅜ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 이영화존잼쓰ㅜㅜㅜㅜㅜㅜㅜㅜ', num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdp\\anaconda3\\envs\\NLP_38\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt = Twitter()\n",
    "twt.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기에 사전 추가하기\n",
    "twt.add_dictionary(words='은경이', tag='Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('은경이', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('사무실', 'Noun'),\n",
       " ('로', 'Josa'),\n",
       " ('갔습니다', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.pos('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
