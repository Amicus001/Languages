{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환신경망 RNN <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1\n",
    "NUM_LAYERS = 1\n",
    "SQ_LENGTH = 3\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "#데이터 및 초기 Hidden State       \n",
    "input = torch.randn(1, 3, 10)        #  입력데이터(배치크기, 시퀀스 길이, 피처 길이)\n",
    "h0 = torch.randn(1, 3, 1)          # 히든 초기값\n",
    "\n",
    "#첫번째 히든스테이트 초기값\n",
    "rnn = nn.RNN(10,1,1)              # RNN 인스턴스 생성하기\n",
    "\n",
    "#RNN출력\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 설계 : 다층 RNN, 층 2개\n",
    "# Input\n",
    "\n",
    "### 시퀀스 : 한 문장을 구성하는 단어 수\n",
    "### 피처   : 한 단어를 표현하는 숫자\n",
    "#입력 초기 텐서들\n",
    "input = torch.randn(BATCH_SIZE, SQ_LENGTH, 10) # 입력 데이터(배치 크기, 시퀀스 길이, 피처 길이)\n",
    "h0 = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE) # 히든 초기화(배치 크기 1, 시퀀스 길이 1, 은닉 상태 벡터 크기 1) 여기서 배치사이즈는 시퀀스 값을 따라간다. 양방향\n",
    "                        # 양방향* 층수, 배치크기, 히든 갯수. \n",
    "#RNN 인스턴스 \n",
    "rnn = nn.RNN(10,HIDDEN_SIZE,NUM_LAYERS, batch_first=True) #입력 텐서 차원 : 배치 크기-시퀀스 길이-특성 벡터 크기 순으로 배치됨\n",
    "                                       #batch_first = false: 차원 순서가 배치 크기 - 시퀀스 길이 - 특성 벡터 크기 순으로 배치. \n",
    "# 출력 텐서들\n",
    "output2, hn2 = rnn(input, h0) # RNN에 입력 데이터, 초기 은닉 텐서 제공하여 계산 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INPUT DATA]: torch.Size([1, 3, 10]), DIM = 3\n",
      "torch.Size([1, 3, 1]) torch.Size([1, 3, 1]) 3\n"
     ]
    }
   ],
   "source": [
    "print(f'[INPUT DATA]: {input.shape}, DIM = {input.ndim}')\n",
    "print(output2.shape, hn.shape, output2.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rnn 모델의 속성 출력하기\n",
    "print(f'[ALL_WEIGHTS] :{len(rnn.all_weights)}')\n",
    "print(rnn.all_weights)\n",
    "\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f'======>[{name}]\\n{param}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN PARAMS\n",
      "weight_ih_l0\n",
      " Parameter containing:\n",
      "tensor([[ 0.8806, -0.5637, -0.1673,  0.5733, -0.8343,  0.7533,  0.0928,  0.7324,\n",
      "          0.9631,  0.4637]], requires_grad=True)\n",
      "weight_hh_l0\n",
      " Parameter containing:\n",
      "tensor([[-0.5655]], requires_grad=True)\n",
      "bias_ih_l0\n",
      " Parameter containing:\n",
      "tensor([-0.5988], requires_grad=True)\n",
      "bias_hh_l0\n",
      " Parameter containing:\n",
      "tensor([-0.7546], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN PARAMS')\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f'{name}\\n {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNN                                      28\n",
       "=================================================================\n",
       "Total params: 28\n",
       "Trainable params: 28\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 1]), 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RNN 출력 텐서 output\n",
    "\n",
    "output.shape, output.ndim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
