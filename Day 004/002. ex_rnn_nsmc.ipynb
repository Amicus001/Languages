{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [ 한글 데이터셋 RNN 모델 만들기 ] <hr>\n",
    "- Dataset: NSMC from Korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "from  Korpora import Korpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\kdp\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\kdp\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                                굳 ㅋ      1\n",
       "1                               GDNTOPCLASSINTHECLUB      0\n",
       "2             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data loading\n",
    "\n",
    "corpus1 = Korpora.load('nsmc')\n",
    "corpusDF = pd.DataFrame(corpus1.test)\n",
    "\n",
    "corpusDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45000 entries, 33553 to 6838\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    45000 non-null  object\n",
      " 1   label   45000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "## 학습용 / 테스트용 데이터 분리하기\n",
    "\n",
    "trainDF = corpusDF.sample(frac=0.9, random_state=42)\n",
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5000 entries, 9 to 49997\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   label   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 117.2+ KB\n"
     ]
    }
   ],
   "source": [
    "testDF = corpusDF.drop(labels=trainDF.index)\n",
    "testDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 단어사전 생성하기 <hr>\n",
    "\n",
    " - 토큰화 진행하기 -> 형태소 분석기 선택, 분할\n",
    " - 단어사전 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - 1. 토큰화 진행: 문장을 단어로 쪼개기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module importing\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "#tokenizer instance\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "# 자주 쓰는 이상자 정의\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#문장 -> 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in trainDF.text:\n",
    "#     print(okt.morphs(text, stem=True)) # stem=True -> 단어의 어근으로 변형하여 추출하기.\n",
    "#     break\n",
    "\n",
    "train_tokens = [okt.morphs(text, stem=True) for text in trainDF.text]\n",
    "test_tokens = [okt.morphs(text, stem=True) for text in testDF.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN TOKENS] 45000\n",
      "[TEST TOKENS] 5000\n",
      "[TRAIN TOKENS[0]] 19\n",
      "[TEST TOKENS[0]] 18\n"
     ]
    }
   ],
   "source": [
    "print(f'[TRAIN TOKENS] {len(train_tokens)}\\n[TEST TOKENS] {len(test_tokens)}')\n",
    "print(f'[TRAIN TOKENS[0]] {len(train_tokens[0])}\\n[TEST TOKENS[0]] {len(test_tokens[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성하기\n",
    "def build_vocab(corpus, vocab_size, special_tokens):\n",
    "    counter = Counter() # 단어 세는 애\n",
    "\n",
    "    # 단어 / 토큰에 대한 빈도 수 계산\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # 단어 / 어휘 사전 생성하기\n",
    "    vocab = special_tokens\n",
    "\n",
    "    # 빈도수가 높은 단어부터 단어사전에 추가하기. \n",
    "    for token, count in counter.most_common(vocab_size):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = okt\n",
    "# train_tokens = [tokenizer.morphs(review) for review in trainDF.text]\n",
    "# test_tokens = [tokenizer.morphs(review)for review in testDF.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(corpus = train_tokens, vocab_size = 3000, special_tokens= ['<PAD>', '<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VOCAB SIZE] 3002\n",
      "['<PAD>', '<UNK>', '.', '이', '영화', '보다', '하다', '의', '..', '에', '가', '...', '을', '도', '들', ',', '는', '를', '은', '없다', '이다', '있다', '좋다', '?', '너무', '다', '정말', '한', '되다', '재밌다']\n"
     ]
    }
   ],
   "source": [
    "print(f'[VOCAB SIZE] {len(vocab)}\\n{vocab[:30]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - 3. 인코딩 - 디코딩 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 : 문자 -> 숫자\n",
    "encode = {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "# 디코딩 : 숫자 -> 문자\n",
    "decode = {idx: token for idx, token in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰의 문자를 정수로 변환 / 단어사전에 없는 문자 처리\n",
    "\n",
    "UNK_ID = encode.get('<UNK>')\n",
    "train_id = [[encode.get(token, UNK_ID) for token in text]for text in train_tokens]\n",
    "test_id = [[encode.get(token,UNK_ID) for token in text] for text in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰 정수화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - 2. 데이터 구성 단어 수 맞추기 : 패딩\n",
    "    - 단어 수 선정하기\n",
    "    - 선정된 단어 수에 맞게 데이터 조잘: 길면 자르고 짧으면 채운다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 처리 함수\n",
    "## -sentences : 토큰화된 문장 데이터\n",
    "## -max_length : 최대 문장 길이= 문장 1개를 구성하는 단어의 수\n",
    "## -pad : 패딩 처리시 추가될 문자 값\n",
    "## - start : 패딩 시 처리 방향 (기본 : R(오른쪽) 자르기 / 추가하기)\n",
    "\n",
    "def pad_sequences(sentences, max_length, pad, start = 'R'):\n",
    "    result = []\n",
    "    for sen in sentences:\n",
    "        sen = sen[:max_length] if start == 'R' else sen[:-1*max_length] #start 매개변수가 R이면 오른쪽에서 잘라내기\n",
    "        padd_sen = sen + [pad]*(max_length - len(sen)) if start =='R' else ([pad]*(max_length - len(sen)) + sen) # start 매개변수가 R이면 오른쪽부터 패딩 넣기\n",
    "        result.append(padd_sen)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용, 테스트용 데이터 패딩 처리\n",
    "PAD_ID = encode.get('<PAD>')\n",
    "MAX_LENGTH = 32\n",
    "\n",
    "train_id = pad_sequences(train_id, MAX_LENGTH, PAD_ID)\n",
    "test_id = pad_sequences(test_id, MAX_LENGTH, PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN ID] 32\n",
      "[TEST ID] 32\n"
     ]
    }
   ],
   "source": [
    "print(f'[TRAIN ID] {len(train_id[0])}\\n[TEST ID] {len(test_id[0])}') # 0번 원소를 봐야 됨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 학습 준비 <hr>\n",
    "- 데이터로더 준비하기\n",
    "- 학습용 / 테스트용 함수\n",
    "- 모델 클래스 생성하기\n",
    "- 학습 관련 변수 설정: DEVICE, OPTIMIZER, MODEL INSTANCE, EPOCHS, BATCH_SIZE, LOSS FUNCTION, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - 1. Dataloader 만들기\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45000, 32]) torch.Size([45000])\n",
      "torch.Size([5000, 32]) torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "#dataset 생성: List -> TensorDataset\n",
    "# 학습용 데이터셋\n",
    "dataTS = torch.LongTensor(train_id)\n",
    "labelTS = torch.FloatTensor(trainDF.label.values)\n",
    "\n",
    "\n",
    "print(dataTS.shape, labelTS.shape)\n",
    "trainDS = TensorDataset(dataTS, labelTS)\n",
    "\n",
    "\n",
    "# 테스트용 데이터셋\n",
    "testdataTS = torch.LongTensor(test_id)\n",
    "testlabelTS = torch.FloatTensor(testDF.label.values)\n",
    "testDS = TensorDataset(testdataTS, testlabelTS)\n",
    "print(testdataTS.shape, testlabelTS.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trainDL = DataLoader(trainDS, batch_size = BATCH_SIZE, shuffle = True)\n",
    "testDL = DataLoader(testDS, batch_size = BATCH_SIZE, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - 2. 모델 클래스 정의하기\n",
    "- 입력층 : Embedding Layer\n",
    "- 은닉층:  RNN / LSTM layer, Dropout layer\n",
    "- 출력층 : Linear layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CLASSIFIER(nn.Module):\n",
    "    def __init__(self, vocab_size,hidden_dim, embedding_dim, n_layers, dropout=0.5, bidirectional = True, model_type = 'lstm'):\n",
    "        super().__init__()\n",
    "\n",
    "        # 임베딩 설정\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx=0)\n",
    "        \n",
    "        # 모델 지정이 lstm이라면\n",
    "        if model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional= bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True)\n",
    "            \n",
    "        # 모델 지정이 rnn이라면\n",
    "        elif model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                biderectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first= True)\n",
    "            \n",
    "            #bidirectional(양방향 진행) 파라미터가 true라면\n",
    "        if bidirectional == True:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2,1) # (양방향이니까) 리니어층을 두 배로 만들어주고 출력은 하나\n",
    "        else: \n",
    "            self.classifier = nn.Linear(hidden_dim, 1) #false라면\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout) # 드롭아웃: 노드를 랜덤하게 비활성화시킵니다. \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:,-1,:]\n",
    "        last_output = self.dropout(last_output) # dropout은 연산을 안합니다. 비활성화를 시킵니다.\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits ## 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 손실함수 / 최적화함수 정의하기\n",
    "\n",
    "from torch import optim\n",
    "VOCAB_SIZE = len(encode)\n",
    "HIDDEN_DIM = 64\n",
    "EMBEDDING_DIM = 128\n",
    "N_LAYERS = 2\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "S_CLASSIFIER= CLASSIFIER(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, n_layers=N_LAYERS).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(S_CLASSIFIER.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=1, bias=True)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_CLASSIFIER.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 테스트\n",
    "\n",
    "def train(model, dataset, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step,(inputs, labels) in enumerate(dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f'train loss {step}: {np.mean(losses)}')\n",
    "\n",
    "def test(model, dataset, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(torch.eq(yhat,labels).cpu().tolist())\n",
    "\n",
    "        print(f'Val_Loss : {np.mean(losses)}, val_acc:{np.mean(corrects)}')\n",
    "    \n",
    "\n",
    "epoch = 5\n",
    "interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0: 0.682779848575592\n",
      "train loss 500: 0.6863231033146262\n",
      "train loss 1000: 0.6770756258354796\n",
      "Val_Loss : 0.6470910310745239, val_acc:0.65625\n",
      "Val_Loss : 0.5949089825153351, val_acc:0.6875\n",
      "Val_Loss : 0.5449325640996298, val_acc:0.71875\n",
      "Val_Loss : 0.5597177594900131, val_acc:0.71875\n",
      "Val_Loss : 0.5629166007041931, val_acc:0.7125\n",
      "Val_Loss : 0.5427575955788294, val_acc:0.734375\n",
      "Val_Loss : 0.5576777415616172, val_acc:0.7098214285714286\n",
      "Val_Loss : 0.5651804767549038, val_acc:0.7109375\n",
      "Val_Loss : 0.5611190232965682, val_acc:0.7118055555555556\n",
      "Val_Loss : 0.5776222854852676, val_acc:0.696875\n",
      "Val_Loss : 0.5810670337893746, val_acc:0.6903409090909091\n",
      "Val_Loss : 0.5961141362786293, val_acc:0.6692708333333334\n",
      "Val_Loss : 0.5909106387541845, val_acc:0.6778846153846154\n",
      "Val_Loss : 0.5990898204701287, val_acc:0.6674107142857143\n",
      "Val_Loss : 0.6008637448151907, val_acc:0.6708333333333333\n",
      "Val_Loss : 0.59946727193892, val_acc:0.671875\n",
      "Val_Loss : 0.6016421405708089, val_acc:0.6672794117647058\n",
      "Val_Loss : 0.6002427058087455, val_acc:0.6684027777777778\n",
      "Val_Loss : 0.6019654885718697, val_acc:0.662828947368421\n",
      "Val_Loss : 0.6050028666853905, val_acc:0.6625\n",
      "Val_Loss : 0.6050213845003218, val_acc:0.6622023809523809\n",
      "Val_Loss : 0.6061447411775589, val_acc:0.6619318181818182\n",
      "Val_Loss : 0.6021716011607129, val_acc:0.6684782608695652\n",
      "Val_Loss : 0.5978851901988188, val_acc:0.671875\n",
      "Val_Loss : 0.6014434015750885, val_acc:0.67\n",
      "Val_Loss : 0.6041612705359092, val_acc:0.6658653846153846\n",
      "Val_Loss : 0.6024927132659488, val_acc:0.6666666666666666\n",
      "Val_Loss : 0.6066822037100792, val_acc:0.6640625\n",
      "Val_Loss : 0.6048139713961502, val_acc:0.665948275862069\n",
      "Val_Loss : 0.6081234782934188, val_acc:0.6625\n",
      "Val_Loss : 0.6105134785175323, val_acc:0.6602822580645161\n",
      "Val_Loss : 0.6150632360950112, val_acc:0.65625\n",
      "Val_Loss : 0.6134382748242581, val_acc:0.6600378787878788\n",
      "Val_Loss : 0.612592934685595, val_acc:0.6617647058823529\n",
      "Val_Loss : 0.6157962415899549, val_acc:0.6598214285714286\n",
      "Val_Loss : 0.6191923527253999, val_acc:0.6553819444444444\n",
      "Val_Loss : 0.6161050724016653, val_acc:0.6579391891891891\n",
      "Val_Loss : 0.616013447705068, val_acc:0.6595394736842105\n",
      "Val_Loss : 0.6148386574708499, val_acc:0.6586538461538461\n",
      "Val_Loss : 0.6166972450911998, val_acc:0.6578125\n",
      "Val_Loss : 0.613461154263194, val_acc:0.6623475609756098\n",
      "Val_Loss : 0.6145208108992803, val_acc:0.6622023809523809\n",
      "Val_Loss : 0.6150983613590861, val_acc:0.6635174418604651\n",
      "Val_Loss : 0.6119272708892822, val_acc:0.6683238636363636\n",
      "Val_Loss : 0.6103449410862393, val_acc:0.6708333333333333\n",
      "Val_Loss : 0.6096409883188165, val_acc:0.6698369565217391\n",
      "Val_Loss : 0.6100561783668843, val_acc:0.6722074468085106\n",
      "Val_Loss : 0.610091220587492, val_acc:0.673828125\n",
      "Val_Loss : 0.6100604217879626, val_acc:0.6728316326530612\n",
      "Val_Loss : 0.6107966113090515, val_acc:0.67125\n",
      "Val_Loss : 0.6108376675961065, val_acc:0.6727941176470589\n",
      "Val_Loss : 0.6102451338217809, val_acc:0.6736778846153846\n",
      "Val_Loss : 0.6110499422505217, val_acc:0.6727594339622641\n",
      "Val_Loss : 0.6098874001591293, val_acc:0.6741898148148148\n",
      "Val_Loss : 0.6119021220640702, val_acc:0.6721590909090909\n",
      "Val_Loss : 0.6114411779812404, val_acc:0.6735491071428571\n",
      "Val_Loss : 0.6109305264657003, val_acc:0.6737938596491229\n",
      "Val_Loss : 0.6104734755795578, val_acc:0.6734913793103449\n",
      "Val_Loss : 0.6116954381183043, val_acc:0.6716101694915254\n",
      "Val_Loss : 0.6112617452939352, val_acc:0.6703125\n",
      "Val_Loss : 0.6122538193327481, val_acc:0.6685450819672131\n",
      "Val_Loss : 0.6157134242596165, val_acc:0.6648185483870968\n",
      "Val_Loss : 0.615897452074384, val_acc:0.6651785714285714\n",
      "Val_Loss : 0.6169902868568897, val_acc:0.6640625\n",
      "Val_Loss : 0.6181005221146804, val_acc:0.6639423076923077\n",
      "Val_Loss : 0.6190594841133464, val_acc:0.6633522727272727\n",
      "Val_Loss : 0.6185378058632808, val_acc:0.664179104477612\n",
      "Val_Loss : 0.6178072734790689, val_acc:0.6654411764705882\n",
      "Val_Loss : 0.6175786109938137, val_acc:0.6653079710144928\n",
      "Val_Loss : 0.6161055879933494, val_acc:0.665625\n",
      "Val_Loss : 0.6164748240524615, val_acc:0.664612676056338\n",
      "Val_Loss : 0.6177148239480125, val_acc:0.6640625\n",
      "Val_Loss : 0.6170262620873648, val_acc:0.6652397260273972\n",
      "Val_Loss : 0.6179349148595655, val_acc:0.6651182432432432\n",
      "Val_Loss : 0.6188491892814636, val_acc:0.6645833333333333\n",
      "Val_Loss : 0.6192783901565954, val_acc:0.662828947368421\n",
      "Val_Loss : 0.6192108911353272, val_acc:0.6623376623376623\n",
      "Val_Loss : 0.6191744964856368, val_acc:0.6618589743589743\n",
      "Val_Loss : 0.6194670404059978, val_acc:0.6621835443037974\n",
      "Val_Loss : 0.6193235009908676, val_acc:0.6625\n",
      "Val_Loss : 0.61832594871521, val_acc:0.6647376543209876\n",
      "Val_Loss : 0.6177674401097182, val_acc:0.6642530487804879\n",
      "Val_Loss : 0.6173576138105737, val_acc:0.6649096385542169\n",
      "Val_Loss : 0.6170018421752113, val_acc:0.6640625\n",
      "Val_Loss : 0.616985358911402, val_acc:0.6639705882352941\n",
      "Val_Loss : 0.6173052032326543, val_acc:0.6620639534883721\n",
      "Val_Loss : 0.6169449104659859, val_acc:0.6623563218390804\n",
      "Val_Loss : 0.6167437393556942, val_acc:0.6619318181818182\n",
      "Val_Loss : 0.6164130267132534, val_acc:0.6611657303370787\n",
      "Val_Loss : 0.6167650269137488, val_acc:0.6600694444444445\n",
      "Val_Loss : 0.6172544261911413, val_acc:0.6600274725274725\n",
      "Val_Loss : 0.6182671552119048, val_acc:0.6579483695652174\n",
      "Val_Loss : 0.6175809534647132, val_acc:0.6592741935483871\n",
      "Val_Loss : 0.6178853283537195, val_acc:0.6589095744680851\n",
      "Val_Loss : 0.6172471171931216, val_acc:0.6601973684210526\n",
      "Val_Loss : 0.6176969359318415, val_acc:0.6608072916666666\n",
      "Val_Loss : 0.6168054803130553, val_acc:0.6614046391752577\n",
      "Val_Loss : 0.6173698756159568, val_acc:0.6607142857142857\n",
      "Val_Loss : 0.6180701033033505, val_acc:0.6597222222222222\n",
      "Val_Loss : 0.6185100167989731, val_acc:0.6603125\n",
      "Val_Loss : 0.6179362122375186, val_acc:0.6605816831683168\n",
      "Val_Loss : 0.6168848270294713, val_acc:0.6617647058823529\n",
      "Val_Loss : 0.6164884139033198, val_acc:0.6623179611650486\n",
      "Val_Loss : 0.6156321924466354, val_acc:0.6637620192307693\n",
      "Val_Loss : 0.6151968859490894, val_acc:0.6651785714285714\n",
      "Val_Loss : 0.6160459569040334, val_acc:0.6642099056603774\n",
      "Val_Loss : 0.6172688842933869, val_acc:0.6626752336448598\n",
      "Val_Loss : 0.6161417977677451, val_acc:0.6631944444444444\n",
      "Val_Loss : 0.6160229756197798, val_acc:0.6628440366972477\n",
      "Val_Loss : 0.6155943875963038, val_acc:0.6636363636363637\n",
      "Val_Loss : 0.6149159202704558, val_acc:0.6641328828828829\n",
      "Val_Loss : 0.6146846599876881, val_acc:0.6648995535714286\n",
      "Val_Loss : 0.6152149006328752, val_acc:0.6645464601769911\n",
      "Val_Loss : 0.6153216968502915, val_acc:0.6644736842105263\n",
      "Val_Loss : 0.6149656715600387, val_acc:0.6646739130434782\n",
      "Val_Loss : 0.6138370404469555, val_acc:0.665948275862069\n",
      "Val_Loss : 0.6144377194408678, val_acc:0.6658653846153846\n",
      "Val_Loss : 0.6152060817358858, val_acc:0.6652542372881356\n",
      "Val_Loss : 0.6146234802338255, val_acc:0.6651785714285714\n",
      "Val_Loss : 0.6140866902967294, val_acc:0.6653645833333334\n",
      "Val_Loss : 0.613213095536902, val_acc:0.6655475206611571\n",
      "Val_Loss : 0.6134395587151168, val_acc:0.6649590163934426\n",
      "Val_Loss : 0.6136371316463967, val_acc:0.6653963414634146\n",
      "Val_Loss : 0.6142762103869069, val_acc:0.6645665322580645\n",
      "Val_Loss : 0.6136251723766327, val_acc:0.66475\n",
      "Val_Loss : 0.6136316072846216, val_acc:0.6646825396825397\n",
      "Val_Loss : 0.6140757350940403, val_acc:0.6633858267716536\n",
      "Val_Loss : 0.6151486018206924, val_acc:0.662353515625\n",
      "Val_Loss : 0.6154979132404623, val_acc:0.6623062015503876\n",
      "Val_Loss : 0.6148088274093775, val_acc:0.6632211538461539\n",
      "Val_Loss : 0.6150840295635107, val_acc:0.6629293893129771\n",
      "Val_Loss : 0.616100749508901, val_acc:0.6619318181818182\n",
      "Val_Loss : 0.616020532254886, val_acc:0.6616541353383458\n",
      "Val_Loss : 0.6151315205132784, val_acc:0.6625466417910447\n",
      "Val_Loss : 0.6148064525039107, val_acc:0.6627314814814815\n",
      "Val_Loss : 0.6151170493925319, val_acc:0.6622242647058824\n",
      "Val_Loss : 0.6155024162174141, val_acc:0.6617244525547445\n",
      "Val_Loss : 0.6150427620480026, val_acc:0.6616847826086957\n",
      "Val_Loss : 0.6152576306741014, val_acc:0.6618705035971223\n",
      "Val_Loss : 0.6150963698114668, val_acc:0.6625\n",
      "Val_Loss : 0.6156535203575243, val_acc:0.661790780141844\n",
      "Val_Loss : 0.6152841872732404, val_acc:0.6626320422535211\n",
      "Val_Loss : 0.6156843800644775, val_acc:0.6625874125874126\n",
      "Val_Loss : 0.615672741499212, val_acc:0.6627604166666666\n",
      "Val_Loss : 0.6156015083707612, val_acc:0.6625\n",
      "Val_Loss : 0.6155838733666563, val_acc:0.6622431506849316\n",
      "Val_Loss : 0.615088552439294, val_acc:0.6624149659863946\n",
      "Val_Loss : 0.6158935230326008, val_acc:0.6613175675675675\n",
      "Val_Loss : 0.615184821138446, val_acc:0.6623322147651006\n",
      "Val_Loss : 0.6149662335713705, val_acc:0.663125\n",
      "Val_Loss : 0.6148215816510434, val_acc:0.6634933774834437\n",
      "Val_Loss : 0.6151309676076236, val_acc:0.6632401315789473\n",
      "Val_Loss : 0.6146222423104679, val_acc:0.6633986928104575\n",
      "Val_Loss : 0.6140803888246611, val_acc:0.6637581168831169\n",
      "Val_Loss : 0.6138835683945687, val_acc:0.6641129032258064\n",
      "Val_Loss : 0.6134503728304154, val_acc:0.664863782051282\n",
      "Val_Loss : 0.6145471057314782, val_acc:0.6642\n",
      "train loss 0: 0.5584414601325989\n",
      "train loss 500: 0.5981393587922383\n",
      "train loss 1000: 0.60474444066847\n",
      "Val_Loss : 0.4747086763381958, val_acc:0.8125\n",
      "Val_Loss : 0.5572817325592041, val_acc:0.734375\n",
      "Val_Loss : 0.5613785982131958, val_acc:0.7395833333333334\n",
      "Val_Loss : 0.5272772461175919, val_acc:0.765625\n",
      "Val_Loss : 0.5297438859939575, val_acc:0.75625\n",
      "Val_Loss : 0.5027768909931183, val_acc:0.78125\n",
      "Val_Loss : 0.5031384825706482, val_acc:0.7767857142857143\n",
      "Val_Loss : 0.49417290836572647, val_acc:0.78125\n",
      "Val_Loss : 0.49353548222117954, val_acc:0.78125\n",
      "Val_Loss : 0.5099026709794998, val_acc:0.765625\n",
      "Val_Loss : 0.5141550329598513, val_acc:0.7642045454545454\n",
      "Val_Loss : 0.5224520737926165, val_acc:0.7630208333333334\n",
      "Val_Loss : 0.5229654059960291, val_acc:0.7692307692307693\n",
      "Val_Loss : 0.5201941217694964, val_acc:0.7767857142857143\n",
      "Val_Loss : 0.5333099047342936, val_acc:0.7666666666666667\n",
      "Val_Loss : 0.5278152581304312, val_acc:0.76953125\n",
      "Val_Loss : 0.5253649932496688, val_acc:0.7665441176470589\n",
      "Val_Loss : 0.529646067155732, val_acc:0.7621527777777778\n",
      "Val_Loss : 0.5345323666145927, val_acc:0.7598684210526315\n",
      "Val_Loss : 0.5339660242199897, val_acc:0.7609375\n",
      "Val_Loss : 0.5327213449137551, val_acc:0.7619047619047619\n",
      "Val_Loss : 0.536975777961991, val_acc:0.7556818181818182\n",
      "Val_Loss : 0.5394106157447981, val_acc:0.7527173913043478\n",
      "Val_Loss : 0.5360497335592905, val_acc:0.7513020833333334\n",
      "Val_Loss : 0.5344094932079315, val_acc:0.75\n",
      "Val_Loss : 0.5298058826189774, val_acc:0.7536057692307693\n",
      "Val_Loss : 0.5345199880776582, val_acc:0.75\n",
      "Val_Loss : 0.5350645397390638, val_acc:0.7477678571428571\n",
      "Val_Loss : 0.5373212571801811, val_acc:0.7456896551724138\n",
      "Val_Loss : 0.53620831767718, val_acc:0.74375\n",
      "Val_Loss : 0.5312112752468355, val_acc:0.7469758064516129\n",
      "Val_Loss : 0.5329426014795899, val_acc:0.7431640625\n",
      "Val_Loss : 0.5353000624613329, val_acc:0.7424242424242424\n",
      "Val_Loss : 0.5311516786322874, val_acc:0.7444852941176471\n",
      "Val_Loss : 0.5290229763303484, val_acc:0.7446428571428572\n",
      "Val_Loss : 0.5293109350734286, val_acc:0.7439236111111112\n",
      "Val_Loss : 0.5270153557932055, val_acc:0.7440878378378378\n",
      "Val_Loss : 0.5286049482069517, val_acc:0.743421052631579\n",
      "Val_Loss : 0.5312699675559998, val_acc:0.7411858974358975\n",
      "Val_Loss : 0.5311565771698952, val_acc:0.740625\n",
      "Val_Loss : 0.5307090544119114, val_acc:0.7423780487804879\n",
      "Val_Loss : 0.5303666804518018, val_acc:0.7403273809523809\n",
      "Val_Loss : 0.5278293871602346, val_acc:0.7405523255813954\n",
      "Val_Loss : 0.5279469022696669, val_acc:0.7407670454545454\n",
      "Val_Loss : 0.5286069373289745, val_acc:0.7395833333333334\n",
      "Val_Loss : 0.5279082001551337, val_acc:0.7398097826086957\n",
      "Val_Loss : 0.5254251481370723, val_acc:0.7413563829787234\n",
      "Val_Loss : 0.5277374070137739, val_acc:0.73828125\n",
      "Val_Loss : 0.5275271942420882, val_acc:0.7397959183673469\n",
      "Val_Loss : 0.526018797159195, val_acc:0.740625\n",
      "Val_Loss : 0.5268290977852017, val_acc:0.741421568627451\n",
      "Val_Loss : 0.5251091655630332, val_acc:0.7445913461538461\n",
      "Val_Loss : 0.5228983501218399, val_acc:0.7476415094339622\n",
      "Val_Loss : 0.521671916047732, val_acc:0.7488425925925926\n",
      "Val_Loss : 0.523289270292629, val_acc:0.7482954545454545\n",
      "Val_Loss : 0.5234745533338615, val_acc:0.7483258928571429\n",
      "Val_Loss : 0.5225492496239511, val_acc:0.7478070175438597\n",
      "Val_Loss : 0.5234286764572407, val_acc:0.7478448275862069\n",
      "Val_Loss : 0.5249483666177523, val_acc:0.746292372881356\n",
      "Val_Loss : 0.5237385561068852, val_acc:0.7479166666666667\n",
      "Val_Loss : 0.5230002838080047, val_acc:0.7479508196721312\n",
      "Val_Loss : 0.5245099937723529, val_acc:0.7469758064516129\n",
      "Val_Loss : 0.5284154476627471, val_acc:0.7430555555555556\n",
      "Val_Loss : 0.5278602028265595, val_acc:0.74365234375\n",
      "Val_Loss : 0.5282979873510507, val_acc:0.74375\n",
      "Val_Loss : 0.5269530252976851, val_acc:0.7447916666666666\n",
      "Val_Loss : 0.5254838871422098, val_acc:0.7448694029850746\n",
      "Val_Loss : 0.5241788928123081, val_acc:0.7458639705882353\n",
      "Val_Loss : 0.5251954832802648, val_acc:0.7441123188405797\n",
      "Val_Loss : 0.5251160046883991, val_acc:0.7441964285714285\n",
      "Val_Loss : 0.5253788035520366, val_acc:0.7429577464788732\n",
      "Val_Loss : 0.5275747879511781, val_acc:0.7417534722222222\n",
      "Val_Loss : 0.5277968085791966, val_acc:0.7422945205479452\n",
      "Val_Loss : 0.527237850669268, val_acc:0.7432432432432432\n",
      "Val_Loss : 0.5305598803361257, val_acc:0.7416666666666667\n",
      "Val_Loss : 0.5283315671901954, val_acc:0.7442434210526315\n",
      "Val_Loss : 0.5290503289018359, val_acc:0.7443181818181818\n",
      "Val_Loss : 0.5311554979819518, val_acc:0.7423878205128205\n",
      "Val_Loss : 0.5305136264879492, val_acc:0.7424841772151899\n",
      "Val_Loss : 0.5313785020262003, val_acc:0.7421875\n",
      "Val_Loss : 0.5310301438525871, val_acc:0.7426697530864198\n",
      "Val_Loss : 0.5311084545967055, val_acc:0.7419969512195121\n",
      "Val_Loss : 0.5305303866604725, val_acc:0.7420933734939759\n",
      "Val_Loss : 0.5322702959889457, val_acc:0.7403273809523809\n",
      "Val_Loss : 0.5320124794455136, val_acc:0.7400735294117647\n",
      "Val_Loss : 0.5328647508177646, val_acc:0.7390988372093024\n",
      "Val_Loss : 0.5328245957692465, val_acc:0.7388649425287356\n",
      "Val_Loss : 0.5327819341962988, val_acc:0.73828125\n",
      "Val_Loss : 0.5325494689887829, val_acc:0.7384129213483146\n",
      "Val_Loss : 0.5329656898975372, val_acc:0.7381944444444445\n",
      "Val_Loss : 0.5331274500259986, val_acc:0.7383241758241759\n",
      "Val_Loss : 0.5318250128108523, val_acc:0.7394701086956522\n",
      "Val_Loss : 0.5312916280761841, val_acc:0.7395833333333334\n",
      "Val_Loss : 0.5327287668243368, val_acc:0.738031914893617\n",
      "Val_Loss : 0.5325385241132033, val_acc:0.7381578947368421\n",
      "Val_Loss : 0.5328012173995376, val_acc:0.7379557291666666\n",
      "Val_Loss : 0.5328504169724652, val_acc:0.7380798969072165\n",
      "Val_Loss : 0.5345187530833848, val_acc:0.7369260204081632\n",
      "Val_Loss : 0.5358911400491541, val_acc:0.735479797979798\n",
      "Val_Loss : 0.5353992667794227, val_acc:0.73625\n",
      "Val_Loss : 0.5356933324053736, val_acc:0.7357673267326733\n",
      "Val_Loss : 0.5348480676903444, val_acc:0.7368259803921569\n",
      "Val_Loss : 0.5349438543458587, val_acc:0.7372572815533981\n",
      "Val_Loss : 0.5339396830934745, val_acc:0.7379807692307693\n",
      "Val_Loss : 0.5332967644646054, val_acc:0.7380952380952381\n",
      "Val_Loss : 0.5343998305077823, val_acc:0.7379127358490566\n",
      "Val_Loss : 0.5334960757014907, val_acc:0.7383177570093458\n",
      "Val_Loss : 0.5338583500296982, val_acc:0.7381365740740741\n",
      "Val_Loss : 0.5336838315386291, val_acc:0.7376720183486238\n",
      "Val_Loss : 0.5336603538556532, val_acc:0.7377840909090909\n",
      "Val_Loss : 0.5343442403518401, val_acc:0.7370495495495496\n",
      "Val_Loss : 0.533961834119899, val_acc:0.7374441964285714\n",
      "Val_Loss : 0.5340390326702489, val_acc:0.7372787610619469\n",
      "Val_Loss : 0.534132357752114, val_acc:0.7371162280701754\n",
      "Val_Loss : 0.535158269820006, val_acc:0.7364130434782609\n",
      "Val_Loss : 0.5353007152162749, val_acc:0.7351831896551724\n",
      "Val_Loss : 0.534040032288967, val_acc:0.7355769230769231\n",
      "Val_Loss : 0.533979916471546, val_acc:0.7359639830508474\n",
      "Val_Loss : 0.5347159174310059, val_acc:0.7366071428571429\n",
      "Val_Loss : 0.5353985225160917, val_acc:0.7361979166666667\n",
      "Val_Loss : 0.5351241994495234, val_acc:0.7365702479338843\n",
      "Val_Loss : 0.5357314059968854, val_acc:0.7361680327868853\n",
      "Val_Loss : 0.5356355774693373, val_acc:0.7367886178861789\n",
      "Val_Loss : 0.5352520060635382, val_acc:0.7373991935483871\n",
      "Val_Loss : 0.5358415501117706, val_acc:0.73675\n",
      "Val_Loss : 0.5358128729793761, val_acc:0.7368551587301587\n",
      "Val_Loss : 0.5353256825387008, val_acc:0.7372047244094488\n",
      "Val_Loss : 0.5335242270957679, val_acc:0.73828125\n",
      "Val_Loss : 0.5333456602669502, val_acc:0.7376453488372093\n",
      "Val_Loss : 0.5321419782363451, val_acc:0.7387019230769231\n",
      "Val_Loss : 0.5322136412595064, val_acc:0.7385496183206107\n",
      "Val_Loss : 0.5339427899682161, val_acc:0.7374526515151515\n",
      "Val_Loss : 0.5346115036566454, val_acc:0.7368421052631579\n",
      "Val_Loss : 0.5348050943506297, val_acc:0.7367070895522388\n",
      "Val_Loss : 0.5353022533434408, val_acc:0.7358796296296296\n",
      "Val_Loss : 0.5357933791683, val_acc:0.7355238970588235\n",
      "Val_Loss : 0.5367409449859257, val_acc:0.7344890510948905\n",
      "Val_Loss : 0.5369874495958936, val_acc:0.734375\n",
      "Val_Loss : 0.5365497802230094, val_acc:0.7342625899280576\n",
      "Val_Loss : 0.5375257796474866, val_acc:0.7334821428571429\n",
      "Val_Loss : 0.5370064198125338, val_acc:0.7338209219858156\n",
      "Val_Loss : 0.537844576676127, val_acc:0.7334947183098591\n",
      "Val_Loss : 0.5382807473619501, val_acc:0.7333916083916084\n",
      "Val_Loss : 0.5387747426413827, val_acc:0.7330729166666666\n",
      "Val_Loss : 0.5390006186633275, val_acc:0.7323275862068965\n",
      "Val_Loss : 0.538932571876539, val_acc:0.7324486301369864\n",
      "Val_Loss : 0.539011037471343, val_acc:0.7319302721088435\n",
      "Val_Loss : 0.53848499885282, val_acc:0.7322635135135135\n",
      "Val_Loss : 0.5384568930072272, val_acc:0.7323825503355704\n",
      "Val_Loss : 0.5384448212385178, val_acc:0.7325\n",
      "Val_Loss : 0.5384330009387819, val_acc:0.7324089403973509\n",
      "Val_Loss : 0.5381687175678579, val_acc:0.7325246710526315\n",
      "Val_Loss : 0.5366885679609635, val_acc:0.7340686274509803\n",
      "Val_Loss : 0.5368819761198836, val_acc:0.7337662337662337\n",
      "Val_Loss : 0.5359254667835851, val_acc:0.7346774193548387\n",
      "Val_Loss : 0.5358518144259086, val_acc:0.7339743589743589\n",
      "Val_Loss : 0.5359283651515936, val_acc:0.7338\n",
      "train loss 0: 0.533947765827179\n",
      "train loss 500: 0.5297458579916202\n",
      "train loss 1000: 0.5137172912086521\n",
      "Val_Loss : 0.49589598178863525, val_acc:0.71875\n",
      "Val_Loss : 0.47393499314785004, val_acc:0.71875\n",
      "Val_Loss : 0.4413821796576182, val_acc:0.78125\n",
      "Val_Loss : 0.457930751144886, val_acc:0.765625\n",
      "Val_Loss : 0.4649462640285492, val_acc:0.7625\n",
      "Val_Loss : 0.445932537317276, val_acc:0.7760416666666666\n",
      "Val_Loss : 0.4522846298558371, val_acc:0.7678571428571429\n",
      "Val_Loss : 0.4561881683766842, val_acc:0.76953125\n",
      "Val_Loss : 0.44875079062249923, val_acc:0.7777777777777778\n",
      "Val_Loss : 0.449497377872467, val_acc:0.784375\n",
      "Val_Loss : 0.44572462276978925, val_acc:0.7897727272727273\n",
      "Val_Loss : 0.44324423869450885, val_acc:0.7890625\n",
      "Val_Loss : 0.4475074502137991, val_acc:0.7932692307692307\n",
      "Val_Loss : 0.4362389530454363, val_acc:0.8035714285714286\n",
      "Val_Loss : 0.4313513398170471, val_acc:0.8125\n",
      "Val_Loss : 0.4453086070716381, val_acc:0.806640625\n",
      "Val_Loss : 0.4576081528383143, val_acc:0.7977941176470589\n",
      "Val_Loss : 0.4571654432349735, val_acc:0.7986111111111112\n",
      "Val_Loss : 0.4599038331132186, val_acc:0.7993421052631579\n",
      "Val_Loss : 0.4560074836015701, val_acc:0.8\n",
      "Val_Loss : 0.4544168398493812, val_acc:0.7991071428571429\n",
      "Val_Loss : 0.46297029473564844, val_acc:0.7911931818181818\n",
      "Val_Loss : 0.46795140919478045, val_acc:0.7894021739130435\n",
      "Val_Loss : 0.463709498445193, val_acc:0.7903645833333334\n",
      "Val_Loss : 0.46667553186416627, val_acc:0.79\n",
      "Val_Loss : 0.46876400250654954, val_acc:0.7896634615384616\n",
      "Val_Loss : 0.46905281367125334, val_acc:0.7893518518518519\n",
      "Val_Loss : 0.46988161546843393, val_acc:0.7879464285714286\n",
      "Val_Loss : 0.46936608388506135, val_acc:0.7898706896551724\n",
      "Val_Loss : 0.47499379913012185, val_acc:0.7885416666666667\n",
      "Val_Loss : 0.4765604042237805, val_acc:0.7883064516129032\n",
      "Val_Loss : 0.47760574519634247, val_acc:0.78515625\n",
      "Val_Loss : 0.4757504246451638, val_acc:0.7859848484848485\n",
      "Val_Loss : 0.473235440604827, val_acc:0.7867647058823529\n",
      "Val_Loss : 0.47434606722423006, val_acc:0.7857142857142857\n",
      "Val_Loss : 0.47613510820600724, val_acc:0.7838541666666666\n",
      "Val_Loss : 0.47750074798996384, val_acc:0.7837837837837838\n",
      "Val_Loss : 0.480052345677426, val_acc:0.7837171052631579\n",
      "Val_Loss : 0.4838990523264958, val_acc:0.780448717948718\n",
      "Val_Loss : 0.48242809921503066, val_acc:0.78203125\n",
      "Val_Loss : 0.48271169240881756, val_acc:0.7804878048780488\n",
      "Val_Loss : 0.4828947974102838, val_acc:0.7797619047619048\n",
      "Val_Loss : 0.48061670536218687, val_acc:0.7805232558139535\n",
      "Val_Loss : 0.4829626151106574, val_acc:0.7798295454545454\n",
      "Val_Loss : 0.48508461978700423, val_acc:0.7791666666666667\n",
      "Val_Loss : 0.4805738660304443, val_acc:0.782608695652174\n",
      "Val_Loss : 0.480214369423846, val_acc:0.7819148936170213\n",
      "Val_Loss : 0.47994440980255604, val_acc:0.7805989583333334\n",
      "Val_Loss : 0.4794945984470601, val_acc:0.78125\n",
      "Val_Loss : 0.47846128821372985, val_acc:0.78125\n",
      "Val_Loss : 0.47964109509598973, val_acc:0.7806372549019608\n",
      "Val_Loss : 0.47950579226017, val_acc:0.7806490384615384\n",
      "Val_Loss : 0.47747777542977965, val_acc:0.7824292452830188\n",
      "Val_Loss : 0.4743712384391714, val_acc:0.7841435185185185\n",
      "Val_Loss : 0.47581480253826486, val_acc:0.7835227272727273\n",
      "Val_Loss : 0.47614116807069096, val_acc:0.7840401785714286\n",
      "Val_Loss : 0.47629355704575255, val_acc:0.7839912280701754\n",
      "Val_Loss : 0.4761794574301818, val_acc:0.7855603448275862\n",
      "Val_Loss : 0.4769356073969502, val_acc:0.7838983050847458\n",
      "Val_Loss : 0.47742491513490676, val_acc:0.7848958333333333\n",
      "Val_Loss : 0.47670071144573023, val_acc:0.7848360655737705\n",
      "Val_Loss : 0.4781749056231591, val_acc:0.7832661290322581\n",
      "Val_Loss : 0.47738900165709236, val_acc:0.7827380952380952\n",
      "Val_Loss : 0.47997775580734015, val_acc:0.78125\n",
      "Val_Loss : 0.47939394116401673, val_acc:0.7817307692307692\n",
      "Val_Loss : 0.4788007704597531, val_acc:0.7826704545454546\n",
      "Val_Loss : 0.4799027589719687, val_acc:0.7826492537313433\n",
      "Val_Loss : 0.47851663608761397, val_acc:0.7840073529411765\n",
      "Val_Loss : 0.4775173011897267, val_acc:0.7857789855072463\n",
      "Val_Loss : 0.477105866585459, val_acc:0.7857142857142857\n",
      "Val_Loss : 0.4763356233146829, val_acc:0.7856514084507042\n",
      "Val_Loss : 0.4776856216291587, val_acc:0.7847222222222222\n",
      "Val_Loss : 0.4793145513697846, val_acc:0.7833904109589042\n",
      "Val_Loss : 0.4790489621259071, val_acc:0.7833614864864865\n",
      "Val_Loss : 0.47790065288543704, val_acc:0.78375\n",
      "Val_Loss : 0.47989889195090846, val_acc:0.782483552631579\n",
      "Val_Loss : 0.47878821252228376, val_acc:0.783685064935065\n",
      "Val_Loss : 0.4785797798480743, val_acc:0.7828525641025641\n",
      "Val_Loss : 0.47877734069582784, val_acc:0.7828322784810127\n",
      "Val_Loss : 0.4784218408167362, val_acc:0.78359375\n",
      "Val_Loss : 0.4776666867144314, val_acc:0.7839506172839507\n",
      "Val_Loss : 0.4772954532286016, val_acc:0.7842987804878049\n",
      "Val_Loss : 0.4765218280884157, val_acc:0.7853915662650602\n",
      "Val_Loss : 0.4762973498020853, val_acc:0.7853422619047619\n",
      "Val_Loss : 0.47746661445673777, val_acc:0.7845588235294118\n",
      "Val_Loss : 0.4786787868239159, val_acc:0.7837936046511628\n",
      "Val_Loss : 0.479752898558803, val_acc:0.7834051724137931\n",
      "Val_Loss : 0.47962039201097056, val_acc:0.7830255681818182\n",
      "Val_Loss : 0.48206511273812713, val_acc:0.78125\n",
      "Val_Loss : 0.4804624517758687, val_acc:0.7822916666666667\n",
      "Val_Loss : 0.4812858366704249, val_acc:0.7819368131868132\n",
      "Val_Loss : 0.48149485691733984, val_acc:0.7815896739130435\n",
      "Val_Loss : 0.48201726008487006, val_acc:0.78125\n",
      "Val_Loss : 0.4827129796464392, val_acc:0.78125\n",
      "Val_Loss : 0.48237336779895584, val_acc:0.781578947368421\n",
      "Val_Loss : 0.4820901220664382, val_acc:0.7815755208333334\n",
      "Val_Loss : 0.48304694860251907, val_acc:0.7815721649484536\n",
      "Val_Loss : 0.4834273186873417, val_acc:0.78125\n",
      "Val_Loss : 0.4835065302222666, val_acc:0.7809343434343434\n",
      "Val_Loss : 0.4837543731927872, val_acc:0.780625\n",
      "Val_Loss : 0.4829793309811318, val_acc:0.78125\n",
      "Val_Loss : 0.482600651827513, val_acc:0.7818627450980392\n",
      "Val_Loss : 0.4827290703949419, val_acc:0.7818567961165048\n",
      "Val_Loss : 0.48267376623474634, val_acc:0.7821514423076923\n",
      "Val_Loss : 0.4841401012170882, val_acc:0.78125\n",
      "Val_Loss : 0.484085331829089, val_acc:0.78125\n",
      "Val_Loss : 0.48279681968911786, val_acc:0.7815420560747663\n",
      "Val_Loss : 0.4819226916189547, val_acc:0.7818287037037037\n",
      "Val_Loss : 0.48271440475358873, val_acc:0.78125\n",
      "Val_Loss : 0.4823499023914337, val_acc:0.7815340909090909\n",
      "Val_Loss : 0.48169851249402706, val_acc:0.7826576576576577\n",
      "Val_Loss : 0.48133254583392826, val_acc:0.7823660714285714\n",
      "Val_Loss : 0.4815040236025785, val_acc:0.7820796460176991\n",
      "Val_Loss : 0.4825769333462966, val_acc:0.7804276315789473\n",
      "Val_Loss : 0.48250575065612794, val_acc:0.7801630434782608\n",
      "Val_Loss : 0.4821366783359955, val_acc:0.7796336206896551\n",
      "Val_Loss : 0.48110567606412447, val_acc:0.780715811965812\n",
      "Val_Loss : 0.48010053952871745, val_acc:0.7815148305084746\n",
      "Val_Loss : 0.4794391481315388, val_acc:0.7820378151260504\n",
      "Val_Loss : 0.4789401888847351, val_acc:0.7822916666666667\n",
      "Val_Loss : 0.4788647747236835, val_acc:0.78150826446281\n",
      "Val_Loss : 0.47853617003706633, val_acc:0.7820184426229508\n",
      "Val_Loss : 0.47865158973670585, val_acc:0.7815040650406504\n",
      "Val_Loss : 0.47873569207806743, val_acc:0.78125\n",
      "Val_Loss : 0.47982772064208984, val_acc:0.78\n",
      "Val_Loss : 0.4795356118489826, val_acc:0.7805059523809523\n",
      "Val_Loss : 0.4783684503375076, val_acc:0.781003937007874\n",
      "Val_Loss : 0.47807922144420445, val_acc:0.78076171875\n",
      "Val_Loss : 0.47798372008079704, val_acc:0.7805232558139535\n",
      "Val_Loss : 0.477603261745893, val_acc:0.7814903846153847\n",
      "Val_Loss : 0.4776181355687498, val_acc:0.7814885496183206\n",
      "Val_Loss : 0.4773559213587732, val_acc:0.7819602272727273\n",
      "Val_Loss : 0.47687256201765593, val_acc:0.7824248120300752\n",
      "Val_Loss : 0.475594531021901, val_acc:0.7833488805970149\n",
      "Val_Loss : 0.4762071373286071, val_acc:0.7831018518518519\n",
      "Val_Loss : 0.47780413333983984, val_acc:0.7819393382352942\n",
      "Val_Loss : 0.4775313545752616, val_acc:0.781478102189781\n",
      "Val_Loss : 0.47875677092351776, val_acc:0.7805706521739131\n",
      "Val_Loss : 0.47871109114276417, val_acc:0.7805755395683454\n",
      "Val_Loss : 0.4788812366979463, val_acc:0.7799107142857142\n",
      "Val_Loss : 0.4802415571736951, val_acc:0.7792553191489362\n",
      "Val_Loss : 0.48000930648454476, val_acc:0.7792693661971831\n",
      "Val_Loss : 0.47922114788235487, val_acc:0.7799388111888111\n",
      "Val_Loss : 0.4785377505338854, val_acc:0.7803819444444444\n",
      "Val_Loss : 0.4801719951218572, val_acc:0.7793103448275862\n",
      "Val_Loss : 0.48095783162606903, val_acc:0.778681506849315\n",
      "Val_Loss : 0.4813461346285684, val_acc:0.7780612244897959\n",
      "Val_Loss : 0.48115759926873286, val_acc:0.778293918918919\n",
      "Val_Loss : 0.48078947019257, val_acc:0.77873322147651\n",
      "Val_Loss : 0.48148138840993243, val_acc:0.77875\n",
      "Val_Loss : 0.4811629414163678, val_acc:0.7787665562913907\n",
      "Val_Loss : 0.4806172220330489, val_acc:0.7789884868421053\n",
      "Val_Loss : 0.4808643296653149, val_acc:0.7785947712418301\n",
      "Val_Loss : 0.4794986369934949, val_acc:0.7796266233766234\n",
      "Val_Loss : 0.4802080087123379, val_acc:0.7790322580645161\n",
      "Val_Loss : 0.48081440172898465, val_acc:0.7790464743589743\n",
      "Val_Loss : 0.4799598474411448, val_acc:0.779\n",
      "train loss 0: 0.5705475807189941\n",
      "train loss 500: 0.46045171830825465\n",
      "train loss 1000: 0.4531211960357386\n",
      "Val_Loss : 0.6532082557678223, val_acc:0.6875\n",
      "Val_Loss : 0.5065377056598663, val_acc:0.765625\n",
      "Val_Loss : 0.4984385867913564, val_acc:0.78125\n",
      "Val_Loss : 0.5269822105765343, val_acc:0.75\n",
      "Val_Loss : 0.4939075827598572, val_acc:0.7625\n",
      "Val_Loss : 0.46689354876677197, val_acc:0.7760416666666666\n",
      "Val_Loss : 0.47151746494429453, val_acc:0.7723214285714286\n",
      "Val_Loss : 0.4565339833498001, val_acc:0.78515625\n",
      "Val_Loss : 0.43186526828342015, val_acc:0.8020833333333334\n",
      "Val_Loss : 0.4260704755783081, val_acc:0.80625\n",
      "Val_Loss : 0.4265486570921811, val_acc:0.8039772727272727\n",
      "Val_Loss : 0.4106299839913845, val_acc:0.8151041666666666\n",
      "Val_Loss : 0.4151066942856862, val_acc:0.8076923076923077\n",
      "Val_Loss : 0.4202862073268209, val_acc:0.8058035714285714\n",
      "Val_Loss : 0.42154187262058257, val_acc:0.8041666666666667\n",
      "Val_Loss : 0.4191442197188735, val_acc:0.80859375\n",
      "Val_Loss : 0.41708790116450367, val_acc:0.8069852941176471\n",
      "Val_Loss : 0.415648464527395, val_acc:0.8090277777777778\n",
      "Val_Loss : 0.4181816115191108, val_acc:0.8108552631578947\n",
      "Val_Loss : 0.41564550772309306, val_acc:0.8140625\n",
      "Val_Loss : 0.4089624335368474, val_acc:0.8199404761904762\n",
      "Val_Loss : 0.41365031694824045, val_acc:0.8167613636363636\n",
      "Val_Loss : 0.4165324160586233, val_acc:0.8152173913043478\n",
      "Val_Loss : 0.41970168116192025, val_acc:0.8138020833333334\n",
      "Val_Loss : 0.42305904686450957, val_acc:0.81625\n",
      "Val_Loss : 0.42480610941465086, val_acc:0.8125\n",
      "Val_Loss : 0.42804010543558335, val_acc:0.8090277777777778\n",
      "Val_Loss : 0.42826641191329273, val_acc:0.8080357142857143\n",
      "Val_Loss : 0.4327678654728265, val_acc:0.8049568965517241\n",
      "Val_Loss : 0.4365018958846728, val_acc:0.803125\n",
      "Val_Loss : 0.4347217943399183, val_acc:0.8044354838709677\n",
      "Val_Loss : 0.4332437734119594, val_acc:0.8056640625\n",
      "Val_Loss : 0.4384597119959918, val_acc:0.7992424242424242\n",
      "Val_Loss : 0.443015229614342, val_acc:0.7987132352941176\n",
      "Val_Loss : 0.4442307995898383, val_acc:0.7973214285714286\n",
      "Val_Loss : 0.44225773670607144, val_acc:0.7986111111111112\n",
      "Val_Loss : 0.44161780178546906, val_acc:0.799831081081081\n",
      "Val_Loss : 0.44059553938476664, val_acc:0.7993421052631579\n",
      "Val_Loss : 0.4425422079288043, val_acc:0.7964743589743589\n",
      "Val_Loss : 0.44085810966789724, val_acc:0.79765625\n",
      "Val_Loss : 0.43855135360868963, val_acc:0.7980182926829268\n",
      "Val_Loss : 0.44073407174575896, val_acc:0.796875\n",
      "Val_Loss : 0.4393609030995258, val_acc:0.7979651162790697\n",
      "Val_Loss : 0.44053836132992397, val_acc:0.7961647727272727\n",
      "Val_Loss : 0.4379927658372455, val_acc:0.7979166666666667\n",
      "Val_Loss : 0.4366827390116194, val_acc:0.798233695652174\n",
      "Val_Loss : 0.4355207989190487, val_acc:0.7998670212765957\n",
      "Val_Loss : 0.4360050767039259, val_acc:0.8001302083333334\n",
      "Val_Loss : 0.4358034361990131, val_acc:0.7997448979591837\n",
      "Val_Loss : 0.43854147046804426, val_acc:0.7975\n",
      "Val_Loss : 0.4384020616610845, val_acc:0.7971813725490197\n",
      "Val_Loss : 0.43892416157401526, val_acc:0.796875\n",
      "Val_Loss : 0.43518323802723075, val_acc:0.7995283018867925\n",
      "Val_Loss : 0.4364519524905417, val_acc:0.7980324074074074\n",
      "Val_Loss : 0.4371647371487184, val_acc:0.7977272727272727\n",
      "Val_Loss : 0.4387103800794908, val_acc:0.7974330357142857\n",
      "Val_Loss : 0.437527907783525, val_acc:0.7987938596491229\n",
      "Val_Loss : 0.4377804873832341, val_acc:0.7984913793103449\n",
      "Val_Loss : 0.43992487783149137, val_acc:0.7976694915254238\n",
      "Val_Loss : 0.4378202152748903, val_acc:0.7994791666666666\n",
      "Val_Loss : 0.44116923286289467, val_acc:0.7976434426229508\n",
      "Val_Loss : 0.4430443396972072, val_acc:0.795866935483871\n",
      "Val_Loss : 0.4400328059518148, val_acc:0.7981150793650794\n",
      "Val_Loss : 0.4399128456134349, val_acc:0.7978515625\n",
      "Val_Loss : 0.4375275861758452, val_acc:0.7995192307692308\n",
      "Val_Loss : 0.43910780210386624, val_acc:0.7978219696969697\n",
      "Val_Loss : 0.4387620743086089, val_acc:0.7985074626865671\n",
      "Val_Loss : 0.4380212743930957, val_acc:0.7991727941176471\n",
      "Val_Loss : 0.4370552601589673, val_acc:0.7993659420289855\n",
      "Val_Loss : 0.43504374346562796, val_acc:0.8\n",
      "Val_Loss : 0.43496499787753734, val_acc:0.8001760563380281\n",
      "Val_Loss : 0.43443760834634304, val_acc:0.8003472222222222\n",
      "Val_Loss : 0.43333395192884416, val_acc:0.8000856164383562\n",
      "Val_Loss : 0.43197999350927974, val_acc:0.8006756756756757\n",
      "Val_Loss : 0.43148050049940745, val_acc:0.8008333333333333\n",
      "Val_Loss : 0.43065526120756803, val_acc:0.8013980263157895\n",
      "Val_Loss : 0.43164268000559375, val_acc:0.8015422077922078\n",
      "Val_Loss : 0.4322621980920816, val_acc:0.8012820512820513\n",
      "Val_Loss : 0.4316784835691693, val_acc:0.8026107594936709\n",
      "Val_Loss : 0.4304775344207883, val_acc:0.803515625\n",
      "Val_Loss : 0.4314376985953178, val_acc:0.8032407407407407\n",
      "Val_Loss : 0.43394713303664834, val_acc:0.8006859756097561\n",
      "Val_Loss : 0.4354103446365839, val_acc:0.7996987951807228\n",
      "Val_Loss : 0.4328420862910293, val_acc:0.8013392857142857\n",
      "Val_Loss : 0.4339270162231782, val_acc:0.8003676470588236\n",
      "Val_Loss : 0.43260212086660915, val_acc:0.8019622093023255\n",
      "Val_Loss : 0.4338083577224578, val_acc:0.8020833333333334\n",
      "Val_Loss : 0.4327729430726983, val_acc:0.8029119318181818\n",
      "Val_Loss : 0.4342884895172012, val_acc:0.8019662921348315\n",
      "Val_Loss : 0.4338147406776746, val_acc:0.8013888888888889\n",
      "Val_Loss : 0.4327850670932413, val_acc:0.801510989010989\n",
      "Val_Loss : 0.43270565715173015, val_acc:0.8016304347826086\n",
      "Val_Loss : 0.4330588878803356, val_acc:0.801747311827957\n",
      "Val_Loss : 0.43261380509493197, val_acc:0.8025265957446809\n",
      "Val_Loss : 0.43176665792339725, val_acc:0.8032894736842106\n",
      "Val_Loss : 0.43047199537977576, val_acc:0.8040364583333334\n",
      "Val_Loss : 0.43120722134703215, val_acc:0.8038015463917526\n",
      "Val_Loss : 0.4298488055260814, val_acc:0.8045280612244898\n",
      "Val_Loss : 0.43071212265828646, val_acc:0.8039772727272727\n",
      "Val_Loss : 0.42969041362404825, val_acc:0.805\n",
      "Val_Loss : 0.4307311603633484, val_acc:0.8050742574257426\n",
      "Val_Loss : 0.4299023821949959, val_acc:0.8051470588235294\n",
      "Val_Loss : 0.43145908704660474, val_acc:0.8046116504854369\n",
      "Val_Loss : 0.43229195862435377, val_acc:0.8043870192307693\n",
      "Val_Loss : 0.43405410264219557, val_acc:0.8038690476190476\n",
      "Val_Loss : 0.4346864879974779, val_acc:0.8036556603773585\n",
      "Val_Loss : 0.4341874828683996, val_acc:0.8037383177570093\n",
      "Val_Loss : 0.4335033425854312, val_acc:0.8041087962962963\n",
      "Val_Loss : 0.43376138655964386, val_acc:0.8038990825688074\n",
      "Val_Loss : 0.4334056010300463, val_acc:0.8042613636363637\n",
      "Val_Loss : 0.43336733464185184, val_acc:0.8040540540540541\n",
      "Val_Loss : 0.4325541589143021, val_acc:0.8046875\n",
      "Val_Loss : 0.43370629530564875, val_acc:0.8033738938053098\n",
      "Val_Loss : 0.4349812793365696, val_acc:0.8029057017543859\n",
      "Val_Loss : 0.4358543153690255, val_acc:0.8021739130434783\n",
      "Val_Loss : 0.43584421444041976, val_acc:0.8022629310344828\n",
      "Val_Loss : 0.4361393307773476, val_acc:0.8028846153846154\n",
      "Val_Loss : 0.43516065975872137, val_acc:0.8040254237288136\n",
      "Val_Loss : 0.43486278535438183, val_acc:0.804359243697479\n",
      "Val_Loss : 0.43454741475482783, val_acc:0.8046875\n",
      "Val_Loss : 0.4353641817146096, val_acc:0.8044938016528925\n",
      "Val_Loss : 0.4358461214626422, val_acc:0.8043032786885246\n",
      "Val_Loss : 0.4350271562977535, val_acc:0.8048780487804879\n",
      "Val_Loss : 0.43650259305873224, val_acc:0.8039314516129032\n",
      "Val_Loss : 0.4367568496465683, val_acc:0.80375\n",
      "Val_Loss : 0.4376278953656318, val_acc:0.8035714285714286\n",
      "Val_Loss : 0.4374887221676158, val_acc:0.8038877952755905\n",
      "Val_Loss : 0.4383642963366583, val_acc:0.80322265625\n",
      "Val_Loss : 0.43822085037249925, val_acc:0.8037790697674418\n",
      "Val_Loss : 0.43726353886035774, val_acc:0.8045673076923077\n",
      "Val_Loss : 0.4367594720070599, val_acc:0.8053435114503816\n",
      "Val_Loss : 0.4371833630809278, val_acc:0.8051609848484849\n",
      "Val_Loss : 0.4376731502606456, val_acc:0.8049812030075187\n",
      "Val_Loss : 0.4370052677704327, val_acc:0.8057369402985075\n",
      "Val_Loss : 0.4394022788162585, val_acc:0.8043981481481481\n",
      "Val_Loss : 0.4391485940226737, val_acc:0.8044577205882353\n",
      "Val_Loss : 0.43879782163748776, val_acc:0.8047445255474452\n",
      "Val_Loss : 0.44040603190660477, val_acc:0.8036684782608695\n",
      "Val_Loss : 0.4396128820644008, val_acc:0.804181654676259\n",
      "Val_Loss : 0.44011597111821177, val_acc:0.8037946428571429\n",
      "Val_Loss : 0.44065159633227274, val_acc:0.8034131205673759\n",
      "Val_Loss : 0.43970913964677866, val_acc:0.804137323943662\n",
      "Val_Loss : 0.43907249296878603, val_acc:0.8046328671328671\n",
      "Val_Loss : 0.4388235193780727, val_acc:0.8046875\n",
      "Val_Loss : 0.43946540509832316, val_acc:0.8040948275862069\n",
      "Val_Loss : 0.43963490579634495, val_acc:0.8045804794520548\n",
      "Val_Loss : 0.4399296970797234, val_acc:0.8039965986394558\n",
      "Val_Loss : 0.4403069173363415, val_acc:0.8038429054054054\n",
      "Val_Loss : 0.4392093833060873, val_acc:0.8043204697986577\n",
      "Val_Loss : 0.43865060021479924, val_acc:0.804375\n",
      "Val_Loss : 0.43838614116836067, val_acc:0.8042218543046358\n",
      "Val_Loss : 0.43761357498404224, val_acc:0.8044819078947368\n",
      "Val_Loss : 0.43730182692505953, val_acc:0.8045343137254902\n",
      "Val_Loss : 0.4371168672457918, val_acc:0.804586038961039\n",
      "Val_Loss : 0.43656776134044895, val_acc:0.8048387096774193\n",
      "Val_Loss : 0.4362597934519633, val_acc:0.8050881410256411\n",
      "Val_Loss : 0.4395855630089523, val_acc:0.8044\n",
      "train loss 0: 0.4382963180541992\n",
      "train loss 500: 0.4237770072059955\n",
      "train loss 1000: 0.41715121820018247\n",
      "Val_Loss : 0.3117455542087555, val_acc:0.90625\n",
      "Val_Loss : 0.3212556838989258, val_acc:0.875\n",
      "Val_Loss : 0.4298582673072815, val_acc:0.8229166666666666\n",
      "Val_Loss : 0.4482443630695343, val_acc:0.8046875\n",
      "Val_Loss : 0.46266262531280516, val_acc:0.8\n",
      "Val_Loss : 0.4612616151571274, val_acc:0.7864583333333334\n",
      "Val_Loss : 0.4529334732464382, val_acc:0.7857142857142857\n",
      "Val_Loss : 0.45712194964289665, val_acc:0.78515625\n",
      "Val_Loss : 0.45956401692496407, val_acc:0.7881944444444444\n",
      "Val_Loss : 0.4511507749557495, val_acc:0.79375\n",
      "Val_Loss : 0.43973998860879376, val_acc:0.8011363636363636\n",
      "Val_Loss : 0.4230099320411682, val_acc:0.8125\n",
      "Val_Loss : 0.4256377174304082, val_acc:0.8052884615384616\n",
      "Val_Loss : 0.43489780596324373, val_acc:0.8058035714285714\n",
      "Val_Loss : 0.43503660559654234, val_acc:0.80625\n",
      "Val_Loss : 0.4291142914444208, val_acc:0.806640625\n",
      "Val_Loss : 0.42296044791446014, val_acc:0.8069852941176471\n",
      "Val_Loss : 0.41731714374489254, val_acc:0.8125\n",
      "Val_Loss : 0.41865064752729314, val_acc:0.8125\n",
      "Val_Loss : 0.41608708947896955, val_acc:0.8140625\n",
      "Val_Loss : 0.4153400900818053, val_acc:0.8154761904761905\n",
      "Val_Loss : 0.4137057621370662, val_acc:0.8153409090909091\n",
      "Val_Loss : 0.4172302614087644, val_acc:0.813858695652174\n",
      "Val_Loss : 0.4150879631439845, val_acc:0.81640625\n",
      "Val_Loss : 0.41133469343185425, val_acc:0.81875\n",
      "Val_Loss : 0.4147447095467494, val_acc:0.8137019230769231\n",
      "Val_Loss : 0.41670647815421774, val_acc:0.8136574074074074\n",
      "Val_Loss : 0.41574268362351824, val_acc:0.8158482142857143\n",
      "Val_Loss : 0.41632658757012464, val_acc:0.8157327586206896\n",
      "Val_Loss : 0.41495490074157715, val_acc:0.815625\n",
      "Val_Loss : 0.4117081867110345, val_acc:0.8185483870967742\n",
      "Val_Loss : 0.41670725028961897, val_acc:0.814453125\n",
      "Val_Loss : 0.412656099507303, val_acc:0.8153409090909091\n",
      "Val_Loss : 0.40862131995313306, val_acc:0.8180147058823529\n",
      "Val_Loss : 0.41100844996316094, val_acc:0.8160714285714286\n",
      "Val_Loss : 0.4109163482983907, val_acc:0.8159722222222222\n",
      "Val_Loss : 0.4090474223768389, val_acc:0.8150337837837838\n",
      "Val_Loss : 0.4116874596005992, val_acc:0.8133223684210527\n",
      "Val_Loss : 0.40884125309112745, val_acc:0.8157051282051282\n",
      "Val_Loss : 0.4085167244076729, val_acc:0.81640625\n",
      "Val_Loss : 0.4081351408144323, val_acc:0.8155487804878049\n",
      "Val_Loss : 0.40647317256246296, val_acc:0.8162202380952381\n",
      "Val_Loss : 0.40459290354750876, val_acc:0.8175872093023255\n",
      "Val_Loss : 0.408715160055594, val_acc:0.8167613636363636\n",
      "Val_Loss : 0.40815920763545566, val_acc:0.8180555555555555\n",
      "Val_Loss : 0.40833085581012396, val_acc:0.8172554347826086\n",
      "Val_Loss : 0.4059641760714511, val_acc:0.8191489361702128\n",
      "Val_Loss : 0.40483870171010494, val_acc:0.8203125\n",
      "Val_Loss : 0.406170590799682, val_acc:0.8195153061224489\n",
      "Val_Loss : 0.40733897030353544, val_acc:0.81875\n",
      "Val_Loss : 0.4044007483650656, val_acc:0.821078431372549\n",
      "Val_Loss : 0.4062975667990171, val_acc:0.8197115384615384\n",
      "Val_Loss : 0.4060821977426421, val_acc:0.8207547169811321\n",
      "Val_Loss : 0.40756468254107014, val_acc:0.8182870370370371\n",
      "Val_Loss : 0.4075252570889213, val_acc:0.81875\n",
      "Val_Loss : 0.40907236827271326, val_acc:0.8191964285714286\n",
      "Val_Loss : 0.4084179945159377, val_acc:0.8196271929824561\n",
      "Val_Loss : 0.4074610738918699, val_acc:0.8189655172413793\n",
      "Val_Loss : 0.40947041370100895, val_acc:0.8177966101694916\n",
      "Val_Loss : 0.408107070128123, val_acc:0.8192708333333333\n",
      "Val_Loss : 0.4071439231028322, val_acc:0.819672131147541\n",
      "Val_Loss : 0.4059030807787372, val_acc:0.8200604838709677\n",
      "Val_Loss : 0.40577997480119976, val_acc:0.8194444444444444\n",
      "Val_Loss : 0.40631305193528533, val_acc:0.818359375\n",
      "Val_Loss : 0.4079251523201282, val_acc:0.8168269230769231\n",
      "Val_Loss : 0.40734856914390216, val_acc:0.8172348484848485\n",
      "Val_Loss : 0.40941450266695734, val_acc:0.816231343283582\n",
      "Val_Loss : 0.4087110052213949, val_acc:0.8175551470588235\n",
      "Val_Loss : 0.4093425338682921, val_acc:0.8170289855072463\n",
      "Val_Loss : 0.4099909705775125, val_acc:0.8160714285714286\n",
      "Val_Loss : 0.4084647625264987, val_acc:0.8169014084507042\n",
      "Val_Loss : 0.4099275428387854, val_acc:0.8172743055555556\n",
      "Val_Loss : 0.41047627795232483, val_acc:0.817208904109589\n",
      "Val_Loss : 0.41066473803004705, val_acc:0.816722972972973\n",
      "Val_Loss : 0.4103862031300863, val_acc:0.8158333333333333\n",
      "Val_Loss : 0.41089937012446553, val_acc:0.8153782894736842\n",
      "Val_Loss : 0.41090571996453523, val_acc:0.814935064935065\n",
      "Val_Loss : 0.41355178409662, val_acc:0.813301282051282\n",
      "Val_Loss : 0.4134772465198855, val_acc:0.8132911392405063\n",
      "Val_Loss : 0.41302228420972825, val_acc:0.813671875\n",
      "Val_Loss : 0.4135189100548073, val_acc:0.8136574074074074\n",
      "Val_Loss : 0.41642037906297824, val_acc:0.8125\n",
      "Val_Loss : 0.4167074108698282, val_acc:0.8121234939759037\n",
      "Val_Loss : 0.4189793538479578, val_acc:0.8113839285714286\n",
      "Val_Loss : 0.41779326831593233, val_acc:0.8113970588235294\n",
      "Val_Loss : 0.4166273069243098, val_acc:0.8117732558139535\n",
      "Val_Loss : 0.4178851092683858, val_acc:0.8110632183908046\n",
      "Val_Loss : 0.4204302535138347, val_acc:0.8107244318181818\n",
      "Val_Loss : 0.4211772542990995, val_acc:0.8107443820224719\n",
      "Val_Loss : 0.4200075603193707, val_acc:0.8107638888888888\n",
      "Val_Loss : 0.423359167772335, val_acc:0.8090659340659341\n",
      "Val_Loss : 0.4213480526662391, val_acc:0.8108016304347826\n",
      "Val_Loss : 0.42177348543879806, val_acc:0.8104838709677419\n",
      "Val_Loss : 0.4220170113951602, val_acc:0.8098404255319149\n",
      "Val_Loss : 0.42055423526387464, val_acc:0.8105263157894737\n",
      "Val_Loss : 0.41996418172493577, val_acc:0.8108723958333334\n",
      "Val_Loss : 0.4179773899083285, val_acc:0.8118556701030928\n",
      "Val_Loss : 0.41809934620954553, val_acc:0.8121811224489796\n",
      "Val_Loss : 0.4182695683204766, val_acc:0.8121843434343434\n",
      "Val_Loss : 0.41984033554792405, val_acc:0.81125\n",
      "Val_Loss : 0.41919525631583565, val_acc:0.8109529702970297\n",
      "Val_Loss : 0.4185714520075742, val_acc:0.8115808823529411\n",
      "Val_Loss : 0.41726582345453284, val_acc:0.8121966019417476\n",
      "Val_Loss : 0.41738844462312186, val_acc:0.8121995192307693\n",
      "Val_Loss : 0.4188483377297719, val_acc:0.8116071428571429\n",
      "Val_Loss : 0.42014760032015025, val_acc:0.8113207547169812\n",
      "Val_Loss : 0.4195657093948293, val_acc:0.8119158878504673\n",
      "Val_Loss : 0.41782165318727493, val_acc:0.8127893518518519\n",
      "Val_Loss : 0.4175797586047321, val_acc:0.8130733944954128\n",
      "Val_Loss : 0.41772700439799915, val_acc:0.8130681818181819\n",
      "Val_Loss : 0.41802899418650447, val_acc:0.8130630630630631\n",
      "Val_Loss : 0.41683886013925076, val_acc:0.8136160714285714\n",
      "Val_Loss : 0.41617637055110085, val_acc:0.8138827433628318\n",
      "Val_Loss : 0.41520695832737703, val_acc:0.8141447368421053\n",
      "Val_Loss : 0.4151269342588342, val_acc:0.8144021739130435\n",
      "Val_Loss : 0.4155110850416381, val_acc:0.8141163793103449\n",
      "Val_Loss : 0.4140643038046666, val_acc:0.8149038461538461\n",
      "Val_Loss : 0.41397599787530254, val_acc:0.8148834745762712\n",
      "Val_Loss : 0.4152090412979366, val_acc:0.8140756302521008\n",
      "Val_Loss : 0.4145178580035766, val_acc:0.81484375\n",
      "Val_Loss : 0.41468230567195197, val_acc:0.8145661157024794\n",
      "Val_Loss : 0.4150893894619629, val_acc:0.8145491803278688\n",
      "Val_Loss : 0.4145568336655454, val_acc:0.8142784552845529\n",
      "Val_Loss : 0.4146922710201433, val_acc:0.8142641129032258\n",
      "Val_Loss : 0.41442912089824674, val_acc:0.814\n",
      "Val_Loss : 0.4136314405098794, val_acc:0.814484126984127\n",
      "Val_Loss : 0.4139125899771067, val_acc:0.8142224409448819\n",
      "Val_Loss : 0.4138239548774436, val_acc:0.813720703125\n",
      "Val_Loss : 0.4121547094149183, val_acc:0.8146802325581395\n",
      "Val_Loss : 0.4120103471554243, val_acc:0.8144230769230769\n",
      "Val_Loss : 0.41331265013636526, val_acc:0.8134541984732825\n",
      "Val_Loss : 0.4121731903516885, val_acc:0.8136837121212122\n",
      "Val_Loss : 0.41157047282484244, val_acc:0.8141447368421053\n",
      "Val_Loss : 0.41263896182401855, val_acc:0.8136660447761194\n",
      "Val_Loss : 0.41339906983905367, val_acc:0.812962962962963\n",
      "Val_Loss : 0.4130761658444124, val_acc:0.8129595588235294\n",
      "Val_Loss : 0.4127879527798534, val_acc:0.8134124087591241\n",
      "Val_Loss : 0.4125765073990476, val_acc:0.8131793478260869\n",
      "Val_Loss : 0.41254650645976443, val_acc:0.8129496402877698\n",
      "Val_Loss : 0.4134847002370017, val_acc:0.8120535714285714\n",
      "Val_Loss : 0.41282410380688117, val_acc:0.8122783687943262\n",
      "Val_Loss : 0.4114759797361535, val_acc:0.8131602112676056\n",
      "Val_Loss : 0.4117964958394324, val_acc:0.8133741258741258\n",
      "Val_Loss : 0.4125028925223483, val_acc:0.8129340277777778\n",
      "Val_Loss : 0.41286980715291255, val_acc:0.8129310344827586\n",
      "Val_Loss : 0.4120762846241259, val_acc:0.8133561643835616\n",
      "Val_Loss : 0.41219891516529783, val_acc:0.8133503401360545\n",
      "Val_Loss : 0.4131657950378753, val_acc:0.8125\n",
      "Val_Loss : 0.4140810524457253, val_acc:0.8118708053691275\n",
      "Val_Loss : 0.4135736666123072, val_acc:0.8120833333333334\n",
      "Val_Loss : 0.4136976424037226, val_acc:0.8122930463576159\n",
      "Val_Loss : 0.41236004311787455, val_acc:0.8133223684210527\n",
      "Val_Loss : 0.41279925279367985, val_acc:0.8131127450980392\n",
      "Val_Loss : 0.4123821076634642, val_acc:0.8133116883116883\n",
      "Val_Loss : 0.41204058835583346, val_acc:0.8137096774193548\n",
      "Val_Loss : 0.4123317245871593, val_acc:0.8141025641025641\n",
      "Val_Loss : 0.4127807717794066, val_acc:0.814\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epoch):\n",
    "    train(S_CLASSIFIER, trainDL, criterion, optimizer, DEVICE, interval)\n",
    "    test(S_CLASSIFIER, testDL, criterion, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
